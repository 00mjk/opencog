
                     Eva Chatbot
                     -----------

Prototype that attaches the chatbot to the behavior trees that
control the Hanson Robotics Eva robot emulator.

The Eva simulator emulates an animated human head, able to see
listen and respond.  There is a fair amount of setup needed to
run this thing:

 * The animation is done in blender; you have to install blender
   and get the blender rig.

 * Communications is done using ROS. You have to install ROS and
   catkin and catkin make to build the ROS messages.

 * Behavior control is done with OpenCog behavior trees.

See the directory
https://github.com/opencog/ros-behavior-scripting
for additional information.  It will give pointers as to how to
install and configure the above pieces.

There is also a debugging version that can be run in IRC only,
without requiring a full hookup to the blender head.

Goals
-----
Goals for this chatbot are:

 * Respond to verbal commands, such as "Turn left" or "Don't
   look at me." (she's always looking at you)

 * Answer queries about internal state, such as "Can you see me?"
   "Did you just smile?"

 * Remember things.  e.g. "My name is X"

 * Conduct a reasonably pleasant conversation.

HOWTO
-----
 * make install -- important to get file path names correct!
 * Start the relex server on port 4444
 * Start the cogita IRC bot
 * Start a guile shell.
 * Enter (load "run-chatbot.scm") at the guile prompt.
 * Talk to her via IRC.  Alternately, use
   (process-query "foo" "This is a test")


General sketch
--------------
The general technical intent here is to somehow associate nouns and
verbs with grounded objects and actions. At the most basic level, this
is relatively "easy": a single word, verb or noun, is associated with a
single action or item. The first serious challenge comes with modifiers:
adjectives and adverbs.

Sentences have a syntactic structure, and it is very tempting to try to
"normalize" these structures, flatten them in some way, and work with
the flattened representations.  I feel that such an effort is doomed in
the end: language has structure for a reason, and one ignores that
structure at one's own peril.

The question then becomes: how to ground complex structure? The answer
that I propose here is use a set of refinements of understanding: what,
in topology, is called a "filter".  Ideally, an utterance should be
understood very precisely: every word in the sentence counts, every
syntactic relation has an effect, every tense, mood and number plays a
role in the understanding of that sentence.  In practice, the robot is
stupid, and will be continually enountering sentences that it only
partly understands.  Thus, the concept of a "filter": the filter is a
collection of increasingly less-precise understandings of an utterance.
All of these "understandings" enclose or wrap or encompass the "true"
meaning of the utterance; just that some understandings are less precise
than others.  The task is to find the most prcise understanding
possible.

For example: given an utterance, perhaps there is some rule that
identifies a single word in that sentence: that rule associates a
grounded meaning to that word.  Perhaps there is another rule that can
identify two words in the sentence, and can simultaneously ground both
in a single coherent context for action.  This second rule is then more
precise than the first.  Whatever meaning there is, whatever action is
to be taken, it is to be taken under the control of the second rule.
Unless there is a third rule that is more precise than the second, in
which case the third rule applies. The "filter" here is simply a chain
of possible interpretations of an utterance, ordered from least to
most precise.

If a precise interpration of a sentence cannot be found, then the
sentence should be factored into parts, and those parts be combined as
best as possible.  If one rule recognizes one word, a verb in the
utterance, and another rule recognizes one word, a noun, but there are
no rules that recognize two words, then one does as est as one can:
the sentence is "factored" into parts.  A learning (guessing) subsystem
does its best to combine the parts into a more precise whole: the more
parts, the narrower and more refined the understanding.  XXX TODO
but how does this combination actually work?

The goal of thinking in terms of filters is to think in terms of
"fallback plans" -- if there is no narrow understanding of an utterance,
we fall back to some other, more vague understanding, picking the most
precise one possible, and somehow combining it with any other partial
understanding from other rules that recognize parts of a sentence.

Context
-------
Rules should be applied only in relation to teh current context.  This
is for two reasons: one is performance: if we know that a rule is not
applicable in a certain context, then we don't waste CPU time in
applying it.  A second resons is that rules can simplify, or factor,
by not requring a context to be a part of its specification.  Its not
clear that the URE can currently handle contexts in this sense. At this
time, contextualized knowledge seems to be a slightly lower priority.

ActionLink, GroundedActionLink
------------------------------
New, experimental link types. These will have the following format:

    ActionLink
       SchemaNode "some verb"   ;;; SchemaNode, or WordNode
       ListLink
          ConceptNode "slot value"   ;;; or a WordNode

This is a kind-of flattened representation of a sentence; alternately, a
simplified form of a sentence.  Some rule will take an input imperative
sentence, and boil it down to this.   Another rule will then search for
a grounding for this.  Groundings are of the form:


    ReferenceLink
       ActionLink
          SchemaNode "some verb"   ;;; SchemaNode, or WordNode
          ListLink
             ConceptNode "slot value"   ;;; or a WordNode
       ExecutionOutputLink
        .....   


Issues
------
Linguistic issues abound.  Consider these sentences: "She looks good",
"She looks left".  Both have essentially the same syntactic parse, but
have very different semantics: the first should generate the dependency
relation _to-be(look, good) while the second should generate the active
_to-do(look, left). We cannot distinguish these from the syntax alone.
What should we do?  The only answer I can think of is to learn by
mutating rules ...

Generalizations: for example, look-rule-1 explicitly looks for the verb
"look". This needs to be replaced by some word-sense disambiguation WSD
stage, so that any synonym of "look" is sufficient.  Even more: any
synonymous command should be enough, not just on the word level, but
generally.
