
                     Eva Chatbot
                     -----------

Prototype tht attaches the chatbot to the behavior trees that
control the Hanson Robotics Eva robot emulator.

The Eva simulator emulates an animated human head, able to see
listen and respond.  There is a fair amount of setup needed to
run this thing:

 * The animation is done in blender; you have to install blender
   and get the blender rig.

 * Communications is done using ROS. You have to install ROS and
   catkin and catkin make to build the ROS messages.

 * Behavior control is done with OpenCog behavior trees.

See the directory
https://github.com/opencog/ros-behavior-scripting
for additional information.  It will give pointers as to how to
install and cofigure the above pieces.

Goals
-----
Goals for this chatbot are:

 * Respond to verbal commands, such as "Turn left" or "Don't
   look at me." (she's always looking at you)

 * Answer queries about internal state, such as "Can you see me?"
   "Did you just smile?"

 * Remember things.  e.g. "My name is X"

 * Conduct a reasonably pleasant conversation.


Issues
------
Linguistic issues abound.  Consider these sentences: "She looks good",
"She looks left".  Both have essentially the same syntactic parse, but
have very different semantics: the first should generate the dependency
relation _to-be(look, good) while the second should generate the active
_to-do(look, left). We cannot distinguish these from the syntax alone.
What should we do?  The only answer I can think of is to learn by
mutating rules ...

Generalizations: for example, look-rule-1 explicitly looks for the verb
"look". This needs to be replaced by some word-sense disambiguation WSD
stage, so that any synonym of "look" is sufficient.  Even more: any
synonymous command should be enough, nt just on the word level, but
generally.
