
Combined language and animation
-------------------------------
Proposal - April 2016 - Linas Vepstas

This document sketches a proposed architecture for combining language
with sensory perception, memory, reasoning and action. Yes, such a
combination is a "holy grail" of AGI.  Yes, there must be 1000 other
proposals for how to do this.  This proposal is very low brow: it
attempts to describe something that is doable by a small number of
programmers, in a short period of time, making use of the existing
OpenCog code base, and, at the same time, result in a system that
is not a train-wreck of hack, but something that could maybe survive
and be improved upon in an incremental fashion.

As such, it will touch base with existing source code within OpenCog,
even if that source code is imperfect or inadequate.  It is up to the
reader to understand what portions of the code need to be improved, and
to do that work.

This document recapitulates many things the Ben talks about, but also
tries to draw attention to those areas where we have deficiencies.
During prototyping, I have discovered large, *major* holes, missing
pieces in the OpenCog infrastructure.  I will attempt to describe these
here. These are areas where "we" (i.e. "Ben") have not invested enough
time to formulate and clarify and design.  Because no one associated
with OpenCog has spent much time thinking about these missing parts,
any answers I might propose are necessarily provisional, flawed and
subject to re-appraisal and redesign.


General overview
----------------
The general strategy is this:
 0. Raw sensory input.
 1. Identify what is being asked (said, perceived)
 2. Assemble a set of candidate answers (responses)
 3. Seek evidence for or against each answer (each action to be taken)
 4. Formulate a reply (perform that action)
 5. Raw motor and animation.

Step 0. is the external environmental: video frames, audio. It includes
all the software needed to digest this input and extract some
preliminary, low-level features from it: the words that were spoken,
the intensity of the sound, the faces that we can see.  The rest of this
document will say relatively little about this, other than that we need
to improve it.

Step 1. is the integrated sensory system, sometimes called the
"perception synthesizer". Currently, it is extremely limited: the
Relex and R2L subsystem, and a face-detector (face tracker).  The
sensory system needs to be greatly expanded. See Appendix A for some
relatively simple/easy things we could do to improve the situation.

Note that R2L provides varying levels of text analsysis: the raw words
in the sentence, part-of-speech markup, syntactic analysis,
logic-analysis, identification of a handful of speech acts. It would be
nice to have sound-level and timing information to go with this (see
Appendix A for more).

The point of step 1. is to pull together data from disparate sensory
systems, pre-digest it to pull out some basic abstracted elements
(words, syntax, visible faces) and the timing info for events.

This document will be mostly about steps 2 and 3. Technologies like
PLN and OpenPsi kick in at this location.

Step 4. is what we've loosely been terming "the action orchestrator"
(AO): a place where a small number of responses are converted into
actions: for example: raise hand, smile, and say hello. Notice that
all three of these actions are still "integrated", and have to play
out on the same time line in a coordinated fashion.  Step 4 is where
these get dispatched to be performed.

Step 5. is the actual performance of movements, animations, speech.
It includes the text-to-speech subsystem (TTS) and the blender
animation system.  We have these subsystems currently, they sort-of
work, they could be improved.  This docuement will mostly ignore
the stuff in step 5.

The goal
--------
The goal of this document is to sketch how steps 2 and 3 might work,
and how they might interact with steps 1 and 4.  It will be a mish-mash
description of the prototype code that has been implemented, together
with aspirations about what should be or could be done.

The goal is also to delineate the modules and subsystems that comprise
steps 2 and 3 in such a way that they can be (a) understood sufficiently
clearly to be converted into actual working code, (b) designed in such
a way that future expansion minimizes how much redesign is required,
(c) propose a system that can be prototyped in weeks or months,
(actually, a prototype exists already; it needs expansion) (d) be
kept in continuous-operation, continuous-demo mode, avoiding major
down-times for system overhaul (e) provide an indicator or path to
integrating learning and increasing automation.

Note that step (e) is absolutely vital, and very easy to loose track of
and sideline.  A "temporary", "short-term" goal or substitute for (e) is
(f) allow artists, directors and creatives to alter and script
behaviors.

The existing code base (March/April 2016)
-----------------------------------------


Appendix A - Expanding sensory input
------------------------------------
The sensory ninputs to the system need to be expanded.  Here is a list
of some relatively easy and useful things that could be done.

 * Sound intensity: is the room noisy and loud, or is it quiet?
 * Sudden changes: Was there a loud bang/crash just now?
 * Voices. Are many people talking all at once?
 * Nature of background noise: is it crowd noise? is it applause?
   Is it laughter?  Whistling, jeering? Is the audience/crowd
   whispering?  Did a previsouly noisy room suddenly become still?
   Time stamps for all such events.
 * Conversational pauses:  did the speaker/speech come to an end
   or a pause?  Is speaker saying "umm", "uhh", or other
   conversational-pause device?

Speech-to-text system needs:
 * Provide timestamps for words, i.e. to indicate: is speaker talking
   quickly, slowly?
 * Is speaker talking quietly or loud?  Shouting?
 * General affect perception: angry speech? gentle speech? Falling or
   rising tones?
 * Sex perception: is speaker male or female?

A rough cut for all of the above is surely not that hard: we don't need
a hyper-engineered system right now: we need more than zero, for any of
the above.

Visual system needs:
 * Identify faces as known or unknown faces
 * Identify sex of speaker.
 * Identify children (perhaps from height).
 * Identify direction that person/people are facing: are they facing
   us (the camera)? Are they at 3/4ths face? at 1/2 face (looking
   sideways, turned 90 degrees to us)? turned completely away?
 * Are they making eye contact?
 * Are thier lips moving? i.e. visually, might they be talking?
 * Are there gestures of surprise (lips parting open, suddenly)?
 * Are the still, or animated? Bouncing with excitement or bored?
   Even a simple measure of visual flow will do: a general rate of
   movement, amplitude of movment.
 * Are we looking at a large crowd?  Is the crowd agitated or still?
 * Are they moving towards or away from us? Are they crowwing sideways?
 * Are they sitting or standing?

Some of these things could be easy: just like we listen for audio noise,
and measure the noise intensity, we could measure "video noise" and
simply report if there is a lot of movement (lots of pixels changing),
or only a little, and report when there are big changes in movement.
Even something as simple as this is more than what we curretnly have.

The End
-------
