
Combined language and animation
-------------------------------
Proposal - April 2016 - Linas Vepstas

This document sketches a proposed architecture for combining language
with sensory perception, memory, reasoning and action. Yes, such a
combination is a "holy grail" of AGI.  Yes, there must be 1000 other
proposals for how to do this.  This proposal is very low brow: it
attempts to describe something that is doable by a small number of
programmers, in a short period of time, making use of the existing
OpenCog code base, and, at the same time, result in a system that
is not a train-wreck of hack, but something that could maybe survive
and be improved upon in an incremental fashion.

As such, it will touch base with existing source code within OpenCog,
even if that source code is imperfect or inadequate.  It is up to the
reader to understand what portions of the code need to be improved, and
to do that work.

This document recapitulates many things the Ben talks about, but also
tries to draw attention to those areas where we have deficiencies.
During prototyping, I have discovered large, *major* holes, missing
pieces in the OpenCog infrastructure.  I will attempt to describe these
here. These are areas where "we" (i.e. "Ben") have not invested enough
time to formulate and clarify and design.  Because no one associated
with OpenCog has spent much time thinking about these missing parts,
any answers I might propose are necessarily provisional, flawed and
subject to re-appraisal and redesign.


General overview
----------------
The general strategy is this:
 0. Raw sensory input.
 1. Identify what is being asked (said, perceived)
 2. Assemble a set of candidate answers (responses)
 3. Seek evidence for or against each answer (each action to be taken)
 4. Formulate a reply (perform that action)
 5. Raw motor and animation.

Step 0. is the external environmental: video frames, audio. It includes
all the software needed to digest this input and extract some
preliminary, low-level features from it: the words that were spoken,
the intensity of the sound, the faces that we can see.  The rest of this
document will say relatively little about this, other than that we need
to improve it.

Step 1. is the integrated sensory system, sometimes called the
"perception synthesizer". Currently, it is extremely limited: the
Relex and R2L subsystem, and a face-detector (face tracker).  The
sensory system needs to be greatly expanded. See Appendix A for some
relatively simple/easy things we could do to improve the situation.

Note that R2L provides varying levels of text analsysis: the raw words
in the sentence, part-of-speech markup, syntactic analysis,
logic-analysis, identification of a handful of speech acts. It would be
nice to have sound-level and timing information to go with this (see
Appendix A for more).

The point of step 1. is to pull together data from disparate sensory
systems, pre-digest it to pull out some basic abstracted elements
(words, syntax, visible faces) and the timing info for events.

This document will be mostly about steps 2 and 3. Technologies like
PLN and OpenPsi kick in at this location.

Step 4. is what we've loosely been terming "the action orchestrator"
(AO): a place where a small number of responses are converted into
actions: for example: raise hand, smile, and say hello. Notice that
all three of these actions are still "integrated", and have to play
out on the same time line in a coordinated fashion.  Step 4 is where
these get dispatched to be performed.

Step 5. is the actual performance of movements, animations, speech.
It includes the text-to-speech subsystem (TTS) and the blender
animation system.  We have these subsystems currently, they sort-of
work, they could be improved.  This docuement will mostly ignore
the stuff in step 5.

The goal
--------
The goal of this document is to sketch how steps 2 and 3 might work,
and how they might interact with steps 1 and 4.  It will be a mish-mash
description of the prototype code that has been implemented, together
with aspirations about what should be or could be done.

The goal is also to delineate the modules and subsystems that comprise
steps 2 and 3 in such a way that they can be (a) understood sufficiently
clearly to be converted into actual working code, (b) designed in such
a way that future expansion minimizes how much redesign is required,
(c) propose a system that can be prototyped in weeks or months,
(actually, a prototype exists already; it needs expansion) (d) be
kept in continuous-operation, continuous-demo mode, avoiding major
down-times for system overhaul (e) provide an indicator or path to
integrating learning and increasing automation.

Note that step (e) is absolutely vital, and very easy to loose track of
and sideline.  A "temporary", "short-term" goal or substitute for (e) is
(f) allow artists, directors and creatives to alter and script
behaviors.

Overall design philosophy
-------------------------
There is an overall design philosophy that is meant to be adhered to, in
the implementation of this system.  This philosphy has several parts to
it:

A) Attempt to describe all possible actions as "rules".  These rules
   are probably best expressed as ImplicationLinks, with an antecedent
   that describes the situation in which the rule applies, and a
   consequent describing what should be done.

B) The set of rules to be applied in any given situation is done by
   performing a "fuzzy" search, to see how well a given antecedent
   matches the current state of the system.

C) There should be a graceful fallback, so that if there are no rules
   that seem to apply to the given situation, then perhaps something
   more general can be performed/applied.  For example, if some sentence
   is not completely understood, but some of the words or phrases in
   it can be understood, then processing continues with those.

D) Point C) suggests that there should almost always be more than one
   rule that applies to the given situation.  Multiple rules provide
   choices and alternatives, but also are a challenge in terms of
   ranking priorities.

E) Point D) raises questions of how to rank rules.  Should fuzzy-overlap
   be used? Should the PLN TV rules be used? Should total mutal
   information be maximized?  Should some Bayesian or possibly Markovian
   (e.g. HMM) model be used?  Some neural-net approahc? Something
   involving attention allocation? Some combination of the above?
   The claim here is that such ranking and choosing is an open research
   topic, and thus commiting to an architecture that forces an answer,
   or restricts possibilities, is premature.

F) Because there are multiple stages of processing, this implies that
   the rules obtained via points B) thru E) have to be chained. The
   current chainer does not obviously provide the needed generality.
   Thus, it seems that chaining needs to be ad-hoc, for now.

G) Point C) suggests that the rules should be as general as possible,
   and should avoid focusing on implementation details.  For example,
   a rule meant to understand a sentence should NOT include a hook
   to grab the current sentence: instead, the rule should trust that
   the processing pipeline is feeding it the correct sentence at the
   correct time.

H) Point G) suggests that the rules should be ImplicationLinks and not
   BindLinks or GetLinks. Its the job of the rule-driver to convert
   these "soft, abstract" ImplicationLinks into explicit BindLinks
   to perform some action.  The rule-driver deals with the specifics
   of pushing the data through the rules.

More about points C), D), E): For example: maybe some parts of a single
rule need to be applied strictly, while other parts are very fuzzy. We
don't currently have a way of defining "strict" and "loose" portions of
a fuzzy match.  (Note that, in genetics, there are both highly-conserved
gene sequences, and highly-variable sequences.  Thus, the idea of
different amounts of fuzziness seems "natural").

Perhaps its better to have multiple classes of rules, where some rules
demand a very strict or exact match, while others can be very loose and
fuzzy.  Its not clear how to represent strictness/looseness, or how
to combine them.  Should strict/loose be treated as a probability? A
log-probability? Via some PLN-like formulas? Something else?

For the above reasons, I have been shying away from using either
OpenPsi, or the rule-engine, or anything else besides ad-hoc coding:
its not yet clear how to best create rules, and how to best combine
them.

Folded into the above concerns is how to autogenerate or learn new
rules.


The existing code base (March/April 2016)
-----------------------------------------
The current code base uses a five-step process to manage verbal
interactions with the robot.  The stages are:

 1) Obtain input text, identify and dispatch processing based
   on speech type -- process-query/grounded-talk in bot-api.scm
 2) Dispatch imperatives imperative-process

Design notes:
 Step 1) is curently done with scheme code, but it could/should
    use an ImplicationLink to match up the speech-act type to the
    processing that would be performed.  This would allow a single
    sentence to be interpreted as possibly several different
    speech-act types.  More generally, it would fit better into the
long-term
    ; plan to do all recogniztion with ImplicationLinks+rule-engine.

 


Appendix A - Expanding sensory input
------------------------------------
The sensory ninputs to the system need to be expanded.  Here is a list
of some relatively easy and useful things that could be done.

 * Sound intensity: is the room noisy and loud, or is it quiet?
 * Sudden changes: Was there a loud bang/crash just now?
 * Voices. Are many people talking all at once?
 * Nature of background noise: is it crowd noise? is it applause?
   Is it laughter?  Whistling, jeering? Is the audience/crowd
   whispering?  Did a previsouly noisy room suddenly become still?
   Time stamps for all such events.
 * Conversational pauses:  did the speaker/speech come to an end
   or a pause?  Is speaker saying "umm", "uhh", or other
   conversational-pause device?

Speech-to-text system needs:
 * Provide timestamps for words, i.e. to indicate: is speaker talking
   quickly, slowly?
 * Is speaker talking quietly or loud?  Shouting?
 * General affect perception: angry speech? gentle speech? Falling or
   rising tones?
 * Sex perception: is speaker male or female?

A rough cut for all of the above is surely not that hard: we don't need
a hyper-engineered system right now: we need more than zero, for any of
the above.

Visual system needs:
 * Identify faces as known or unknown faces
 * Identify sex of speaker.
 * Identify children (perhaps from height).
 * Identify direction that person/people are facing: are they facing
   us (the camera)? Are they at 3/4ths face? at 1/2 face (looking
   sideways, turned 90 degrees to us)? turned completely away?
 * Are they making eye contact?
 * Are thier lips moving? i.e. visually, might they be talking?
 * Are there gestures of surprise (lips parting open, suddenly)?
 * Are the still, or animated? Bouncing with excitement or bored?
   Even a simple measure of visual flow will do: a general rate of
   movement, amplitude of movment.
 * Are we looking at a large crowd?  Is the crowd agitated or still?
 * Are they moving towards or away from us? Are they crowwing sideways?
 * Are they sitting or standing?

Some of these things could be easy: just like we listen for audio noise,
and measure the noise intensity, we could measure "video noise" and
simply report if there is a lot of movement (lots of pixels changing),
or only a little, and report when there are big changes in movement.
Even something as simple as this is more than what we curretnly have.

The End
-------
